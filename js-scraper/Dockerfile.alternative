# JS Scraper Service Dockerfile - Alternative Version
FROM node:22-alpine

# Install Chrome dependencies, build tools, and cron
RUN apk add --no-cache \
    chromium \
    nss \
    freetype \
    freetype-dev \
    harfbuzz \
    ca-certificates \
    ttf-freefont \
    python3 \
    make \
    g++ \
    gcc \
    libc-dev \
    linux-headers \
    pkgconfig \
    libusb-dev \
    eudev-dev \
    dcron \
    busybox-suid

# Tell Puppeteer to skip installing Chromium. We'll be using the installed package.
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true \
    PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser

WORKDIR /app

# Install dependencies
COPY package*.json ./
COPY yarn.lock* ./

# Use yarn if yarn.lock exists, otherwise use npm
RUN if [ -f yarn.lock ]; then \
        yarn install --frozen-lockfile --production; \
    else \
        npm install --omit=dev; \
    fi

# Copy source code
COPY . .

# Create log directory
RUN mkdir -p /var/log/js-scraper

# Copy and make scripts executable
COPY start-all-scrapers.sh /app/start-all-scrapers.sh
COPY cron-scraper.sh /app/cron-scraper.sh
RUN chmod +x /app/start-all-scrapers.sh
RUN chmod +x /app/cron-scraper.sh

# Create non-root user
RUN addgroup -g 1001 -S nodejs
RUN adduser -S scraper -u 1001

# Change ownership of app directory and log directory
RUN chown -R scraper:nodejs /app
RUN chown -R scraper:nodejs /var/log/js-scraper

# Set up cron job to run every 3 hours (as root, then switch user)
RUN echo "0 */3 * * * /app/cron-scraper.sh >> /var/log/js-scraper/cron.log 2>&1" | crontab -

# Switch to non-root user
USER scraper

# Expose port (if needed)
EXPOSE 3003

# Start cron daemon and run initial scraping
CMD ["sh", "-c", "echo 'ðŸš€ Starting Wojat JS Scraper Service with 3-hour schedule...' && /app/start-all-scrapers.sh && crond -f -L /var/log/js-scraper/cron.log"]
